{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df7fefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "66dc923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    'person': 0,\n",
    "    'bird': 1,\n",
    "    'cat': 2,\n",
    "    'cow': 3,\n",
    "    'dog': 4,\n",
    "    'horse': 5,\n",
    "    'sheep': 6,\n",
    "    'aeroplane': 7,\n",
    "    'bicycle': 8,\n",
    "    'boat': 9,\n",
    "    'bus': 10,\n",
    "    'car': 11,\n",
    "    'motorbike': 12,\n",
    "    'train': 13,\n",
    "    'bottle': 14,\n",
    "    'chair': 15,\n",
    "    'diningtable': 16,\n",
    "    'pottedplant': 17,\n",
    "    'sofa': 18,\n",
    "    'tvmonitor': 19\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e3cd8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_target_tensor(num_classes, annotation_dict):\n",
    "    # Extract image size information\n",
    "    width = int(annotation_dict['annotation']['size']['width'])\n",
    "    height = int(annotation_dict['annotation']['size']['height'])\n",
    "\n",
    "    # Extract bounding box information\n",
    "    tensor_categories = torch.zeros((20, 224, 224))\n",
    "    for obj in annotation_dict['annotation']['object']:\n",
    "        xmin = int((int(obj['bndbox']['xmin']) / width) * 224)\n",
    "        ymin = int((int(obj['bndbox']['ymin']) / height) * 224)\n",
    "        xmax = int((int(obj['bndbox']['xmax']) / width) * 224)\n",
    "        ymax = int((int(obj['bndbox']['ymax']) / height) * 224)\n",
    "        tensor_categories[class_mapping[obj['name']], xmin:xmax+1, ymin:ymax+1] = 1\n",
    "        \n",
    "    return tensor_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a750ea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate your weakly supervised dataset\n",
    "# YourWeaklySupervisedDataset should provide images and their weak annotations\n",
    "# You need to implement this dataset class\n",
    "num_classes = 20\n",
    "# Create a DataLoader for training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: to_target_tensor(num_classes, x))\n",
    "])\n",
    "train_dataset = VOCDetection(root='./data', year='2012', image_set='train', download=False, transform=transform, target_transform=target_transform)\n",
    "print(train_dataset[0][1].shape)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a8c3f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the U-Net model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        resnet18 = models.resnet18(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*list(resnet18.children())[:-2])\n",
    "        #self.encoder = nn.Sequential(\n",
    "            #nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            #models.resnet18(pretrained=True),\n",
    "            #nn.ReLU(inplace=True),\n",
    "        #)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, num_classes, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7342d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model\n",
    "model = UNet(num_classes=20).to(device)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7db13a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0638\n",
      "Epoch [2/10], Loss: 1.7075\n",
      "Epoch [3/10], Loss: 1.0358\n",
      "Epoch [4/10], Loss: 1.0432\n",
      "Epoch [5/10], Loss: 2.1106\n",
      "Epoch [6/10], Loss: 0.9387\n",
      "Epoch [7/10], Loss: 1.6952\n",
      "Epoch [8/10], Loss: 1.1847\n",
      "Epoch [9/10], Loss: 2.0589\n",
      "Epoch [10/10], Loss: 1.5336\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for images, annotations in train_loader:\n",
    "        images, annotations = images.to(device), annotations.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Compute your loss based on the weak annotations\n",
    "        loss = criterion(outputs, annotations)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save or use the trained model for inference\n",
    "torch.save(model.state_dict(), 'weakly_supervised_segmentation_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "71e86fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 224, 224])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pic should not have > 4 channels. Got 20 channels.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [95]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(normalized_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Convert to a list of PIL Images\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m pil_images \u001b[38;5;241m=\u001b[39m [ToPILImage()(normalized_tensor[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m)]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Merge the list of images into a single grayscale image\u001b[39;00m\n\u001b[1;32m     12\u001b[0m grayscale_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mmerge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m, pil_images)\n",
      "Input \u001b[0;32mIn [95]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(normalized_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Convert to a list of PIL Images\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m pil_images \u001b[38;5;241m=\u001b[39m [\u001b[43mToPILImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m)]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Merge the list of images into a single grayscale image\u001b[39;00m\n\u001b[1;32m     12\u001b[0m grayscale_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mmerge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m, pil_images)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:227\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m \n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py:271\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;66;03m# check number of channels\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should not have > 4 channels. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m channels.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pic, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m}:\n",
      "\u001b[0;31mValueError\u001b[0m: pic should not have > 4 channels. Got 20 channels."
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "from PIL import Image\n",
    "\n",
    "tensor = model(train_dataset[0][0].unsqueeze(0)).squeeze(0)\n",
    "normalized_tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "\n",
    "print(normalized_tensor.shape)\n",
    "# Convert to a list of PIL Images\n",
    "pil_images = [ToPILImage()(normalized_tensor[i]) for i in range(20)]\n",
    "\n",
    "# Merge the list of images into a single grayscale image\n",
    "grayscale_image = Image.merge(\"L\", pil_images)\n",
    "\n",
    "# Display or save the resulting grayscale image\n",
    "grayscale_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e204af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
