{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "df7fefdc",
      "metadata": {
        "id": "df7fefdc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.nn.functional import relu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "66dc923d",
      "metadata": {
        "id": "66dc923d"
      },
      "outputs": [],
      "source": [
        "class_mapping = {\n",
        "    'person': 1,\n",
        "    'bird': 2,\n",
        "    'cat': 3,\n",
        "    'cow': 4,\n",
        "    'dog': 5,\n",
        "    'horse': 6,\n",
        "    'sheep': 7,\n",
        "    'aeroplane': 8,\n",
        "    'bicycle': 9,\n",
        "    'boat': 10,\n",
        "    'bus': 11,\n",
        "    'car': 12,\n",
        "    'motorbike': 13,\n",
        "    'train': 14,\n",
        "    'bottle': 15,\n",
        "    'chair': 16,\n",
        "    'diningtable': 17,\n",
        "    'pottedplant': 18,\n",
        "    'sofa': 19,\n",
        "    'tvmonitor': 20,\n",
        "    'background': 0\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e3cd8083",
      "metadata": {
        "id": "e3cd8083"
      },
      "outputs": [],
      "source": [
        "def to_target_tensor(num_classes, annotation_dict):\n",
        "    # Extract image size information\n",
        "    width = int(annotation_dict['annotation']['size']['width'])\n",
        "    height = int(annotation_dict['annotation']['size']['height'])\n",
        "\n",
        "    # Extract bounding box information\n",
        "    tensor_categories = torch.zeros((1, 224, 224))\n",
        "    for obj in annotation_dict['annotation']['object']:\n",
        "        xmin = int((int(obj['bndbox']['xmin']) / width) * 224)\n",
        "        ymin = int((int(obj['bndbox']['ymin']) / height) * 224)\n",
        "        xmax = int((int(obj['bndbox']['xmax']) / width) * 224)\n",
        "        ymax = int((int(obj['bndbox']['ymax']) / height) * 224)\n",
        "        tensor_categories[0, ymin:ymax+1, xmin:xmax+1] = class_mapping[obj['name']]\n",
        "\n",
        "    return tensor_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a750ea35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a750ea35",
        "outputId": "f49d9868-2468-4308-acf5-0f5a0064a837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
            "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n",
            "torch.Size([1, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "# Instantiate your weakly supervised dataset\n",
        "# YourWeaklySupervisedDataset should provide images and their weak annotations\n",
        "# You need to implement this dataset class\n",
        "num_classes = 20\n",
        "# Create a DataLoader for training\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "])\n",
        "target_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: to_target_tensor(num_classes, x))\n",
        "])\n",
        "train_dataset = VOCDetection(root='./data', year='2012', image_set='train', download=True, transform=transform, target_transform=target_transform)\n",
        "print(train_dataset[0][1].shape)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5a8c3f26",
      "metadata": {
        "id": "5a8c3f26"
      },
      "outputs": [],
      "source": [
        "# Define the U-Net model\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        resnet18 = models.resnet18(pretrained=True)\n",
        "        self.encoder = nn.Sequential(*list(resnet18.children())[:-2])\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "FaHGa6RtOoFy",
      "metadata": {
        "id": "FaHGa6RtOoFy"
      },
      "outputs": [],
      "source": [
        "def f_score(pr, gt, beta=1, eps=1e-7, threshold=None, activation='sigmoid'):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        pr (torch.Tensor): A list of predicted elements\n",
        "        gt (torch.Tensor):  A list of elements that are to be predicted\n",
        "        eps (float): epsilon to avoid zero division\n",
        "        threshold: threshold for outputs binarization\n",
        "    Returns:\n",
        "        float: IoU (Jaccard) score\n",
        "    \"\"\"\n",
        "\n",
        "    if activation is None or activation == \"none\":\n",
        "        activation_fn = lambda x: x\n",
        "    elif activation == \"sigmoid\":\n",
        "        activation_fn = torch.nn.Sigmoid()\n",
        "    elif activation == \"softmax2d\":\n",
        "        activation_fn = torch.nn.Softmax2d()\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            \"Activation implemented for sigmoid and softmax2d\"\n",
        "        )\n",
        "\n",
        "    pr = activation_fn(pr)\n",
        "\n",
        "    if threshold is not None:\n",
        "        pr = (pr > threshold).float()\n",
        "\n",
        "\n",
        "    tp = torch.sum(gt * pr)\n",
        "    fp = torch.sum(pr) - tp\n",
        "    fn = torch.sum(gt) - tp\n",
        "\n",
        "    score = ((1 + beta ** 2) * tp + eps) \\\n",
        "            / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + eps)\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    __name__ = 'dice_loss'\n",
        "\n",
        "    def __init__(self, eps=1e-7, activation='sigmoid'):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, y_pr, y_gt):\n",
        "        return 1 - f_score(y_pr, y_gt, beta=1.,\n",
        "                           eps=self.eps, threshold=None,\n",
        "                           activation=self.activation)\n",
        "\n",
        "\n",
        "class BCEDiceLoss(DiceLoss):\n",
        "    __name__ = 'bce_dice_loss'\n",
        "\n",
        "    def __init__(self, eps=1e-7, activation='sigmoid', lambda_dice=1.0, lambda_bce=1.0):\n",
        "        super().__init__(eps, activation)\n",
        "        if activation == None:\n",
        "            self.bce = nn.BCELoss(reduction='mean')\n",
        "        else:\n",
        "            self.bce = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "        self.lambda_dice=lambda_dice\n",
        "        self.lambda_bce=lambda_bce\n",
        "\n",
        "    def forward(self, y_pr, y_gt):\n",
        "        dice = super().forward(y_pr, y_gt)\n",
        "        bce = self.bce(y_pr, y_gt)\n",
        "        return (self.lambda_dice*dice) + (self.lambda_bce* bce)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7342d918",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7342d918",
        "outputId": "92cb5ad2-d8c4-4e29-959f-2b5f29ff2d98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Set your device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = UNet(num_classes=21).to(device)\n",
        "\n",
        "# Define your loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion2 = BCEDiceLoss(eps=1.0, activation=None)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=2, cooldown=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ihWV16CdaDgv",
      "metadata": {
        "id": "ihWV16CdaDgv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7db13a67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "7db13a67",
        "outputId": "a84a0e1b-8948-4983-e19c-04b1c9e54271"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-15be34ea3a28>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Compute your loss based on the weak annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d65232e5f791>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, y_pr, y_gt)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mdice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mbce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_dice\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_bce\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3120\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3122\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for images, annotations in train_loader:\n",
        "        images, annotations = images.to(device), annotations.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        # Compute your loss based on the weak annotations\n",
        "        loss = criterion2(outputs, annotations)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save or use the trained model for inference\n",
        "torch.save(model.state_dict(), 'weakly_supervised_segmentation_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P4vUqmNX--tZ",
      "metadata": {
        "id": "P4vUqmNX--tZ"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import ToPILImage\n",
        "from PIL import Image\n",
        "\n",
        "tensor = model(train_dataset[0][0].unsqueeze(0).to('cuda')).squeeze(0)\n",
        "normalized_tensor = ((tensor - tensor.min()) / (tensor.max() - tensor.min())).cpu().detach().numpy()\n",
        "\n",
        "output = np.argmax(normalized_tensor, axis=0)\n",
        "target = torch.argmax(train_dataset[0][1], dim=0).numpy()\n",
        "print(normalized_tensor.shape)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "axes[0].imshow(output)\n",
        "axes[0].set_title('Image 1')\n",
        "\n",
        "# Display the second image on the second subplot\n",
        "axes[1].imshow(target)\n",
        "axes[1].set_title('Image 2')\n",
        "\n",
        "\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e86fc9",
      "metadata": {
        "id": "71e86fc9",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import ToPILImage\n",
        "from PIL import Image\n",
        "\n",
        "tensor = model(train_dataset[0][0].unsqueeze(0).cuda()).squeeze(0)\n",
        "normalized_tensor = ((tensor - tensor.min()) / (tensor.max() - tensor.min())).cpu().detach().numpy()\n",
        "\n",
        "print(normalized_tensor.shape)\n",
        "\n",
        "fig, axes = plt.subplots(20, 2, figsize=(10, 100))\n",
        "\n",
        "# Plot each normalized grayscale image\n",
        "for i in range(20):\n",
        "    axes[i, 0].imshow(normalized_tensor[i], cmap='gray', aspect='auto')\n",
        "    axes[i, 0].set_title(f'Image {i + 1}')\n",
        "    axes[i, 0].axis('off')  # Turn off axis labels\n",
        "    axes[i, 1].imshow(train_dataset[0][1][i], cmap='gray', aspect='auto')\n",
        "    axes[i, 1].set_title(f'Image {i + 1}')\n",
        "    axes[i, 1].axis('off')  # Turn off axis labels\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5222f96",
      "metadata": {
        "id": "f5222f96"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model_path = \"weakly_supervised_segmentation_model.pth\"\n",
        "checkpoint = torch.load(model_path)\n",
        "# Load the model's state dictionary\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Optionally, you may also load other things such as optimizer state, epoch, etc.\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# epoch = checkpoint['epoch']\n",
        "\n",
        "# Now, you can use the loaded model for inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf98049",
      "metadata": {
        "id": "1bf98049"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
