{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-17T21:41:17.216837400Z",
     "start_time": "2023-12-17T21:41:17.159848Z"
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "class_mapping = {\n",
    "    'person': 0,\n",
    "    'bird': 1,\n",
    "    'cat': 2,\n",
    "    'cow': 3,\n",
    "    'dog': 4,\n",
    "    'horse': 5,\n",
    "    'sheep': 6,\n",
    "    'aeroplane': 7,\n",
    "    'plane': 7,\n",
    "    'bike': 8,\n",
    "    'bicycle':8,\n",
    "    'boat': 9,\n",
    "    'bus': 10,\n",
    "    'car': 11,\n",
    "    'motorbike': 12,\n",
    "    'train': 13,\n",
    "    'bottle': 14,\n",
    "    'chair': 15,\n",
    "    'diningtable': 16,\n",
    "    'table':16,\n",
    "    'pottedplant': 17,\n",
    "    'plant': 17,\n",
    "    'sofa': 18,\n",
    "    'tvmonitor': 19,\n",
    "    'monitor': 19,\n",
    "    'background': 20,\n",
    "    'void': 255\n",
    "}\n",
    "print(class_mapping['bicycle'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T21:41:17.217843700Z",
     "start_time": "2023-12-17T21:41:17.179837300Z"
    }
   },
   "id": "1d689f2920cd0a61"
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "outputs": [],
   "source": [
    "def read_scribble_xml(xml_file):\n",
    "    #  Parse XML into Element Tree\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    #  Read meta data\n",
    "    filename = root.find('filename').text\n",
    "    width = int(root.find('size/width').text)\n",
    "    height = int(root.find('size/height').text)\n",
    "\n",
    "    #  Read all points and assign them to the tensor\n",
    "    tensor_categories = torch.zeros((22, 224, 224))\n",
    "    \n",
    "    \n",
    "    polygons = root.findall('polygon')\n",
    "    for polygon in polygons:\n",
    "        tag = polygon.find('tag').text\n",
    "        points = np.array([(min(int(int(point.find('X').text)/width*224), 223), min(int(int(point.find('Y').text)/height*224),223)) for point in polygon.findall('point')])\n",
    "        tensor_categories[class_mapping[tag], points[:, 1], points[:, 0]] = 1 \n",
    "    \n",
    "    return filename, tensor_categories\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T21:41:17.217843700Z",
     "start_time": "2023-12-17T21:41:17.195837Z"
    }
   },
   "id": "ad03a895eb280c5d"
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "class ScribbleDataset(Dataset):\n",
    "    def __init__(self, image_dir, xml_dir, xml_files, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.xml_files = xml_files\n",
    "        self.transform = transform\n",
    "        self.xml_dir = xml_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xml_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xml_file = self.xml_files[idx]\n",
    "        xml_path = os.path.join(self.xml_dir, xml_file)\n",
    "        image_path = os.path.join(self.image_dir, xml_file.replace(\".xml\", \".jpg\"))\n",
    "\n",
    "        filename, tensor_categories = read_scribble_xml(xml_path)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        sample = {'image': image, 'tensor_category': tensor_categories}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T21:41:17.240838Z",
     "start_time": "2023-12-17T21:41:17.210836300Z"
    }
   },
   "id": "ebc222be7c187739"
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        image, tensor_categories = sample['image'], sample['tensor_category']\n",
    "        \n",
    "        # Convert image to tensor\n",
    "        image = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()])(image)\n",
    "        \n",
    "        return {'image': image, 'tensor_category': tensor_categories}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T21:41:17.240838Z",
     "start_time": "2023-12-17T21:41:17.226836300Z"
    }
   },
   "id": "d01eeb3821ffc4f8"
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "outputs": [],
   "source": [
    "# Set up dataset and dataloader\n",
    "xml_dir = \"scribble\"\n",
    "image_dir = \"train_JPEGImages\"\n",
    "xml_list = os.listdir(xml_dir)\n",
    "transform = transforms.Compose([ToTensor()])\n",
    "train_data, val_data = train_test_split(xml_list, test_size=0.1, random_state=1)\n",
    "train_dataset = ScribbleDataset(image_dir=image_dir, xml_dir=xml_dir, xml_files=train_data, transform=transform)\n",
    "val_dataset = ScribbleDataset(image_dir=image_dir, xml_dir=xml_dir, xml_files=val_data, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T21:41:17.299846200Z",
     "start_time": "2023-12-17T21:41:17.241836900Z"
    }
   },
   "id": "c170f2412f1e9f42"
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "outputs": [],
   "source": [
    "# Define the U-Net model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        resnet18 = models.resnet18(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*list(resnet18.children())[:-2])\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, num_classes, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T21:41:17.310844200Z",
     "start_time": "2023-12-17T21:41:17.260850Z"
    }
   },
   "id": "d85604ddeb079e6c"
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "outputs": [
    {
     "data": {
      "text/plain": "UNet(\n  (encoder): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (decoder): Sequential(\n    (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n    (5): ReLU(inplace=True)\n    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Upsample(scale_factor=2.0, mode='bilinear')\n    (13): ConvTranspose2d(64, 22, kernel_size=(2, 2), stride=(2, 2))\n    (14): ReLU(inplace=True)\n  )\n)"
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize U-Net model\n",
    "in_channels = 3\n",
    "out_channels = 22  # Number of classes\n",
    "model = UNet(out_channels)\n",
    "num_epochs = 10\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T21:41:17.457839Z",
     "start_time": "2023-12-17T21:41:17.271838300Z"
    }
   },
   "id": "8fd934c250a596e8"
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▏         | 16/677 [00:23<16:15,  1.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[401], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# Backward pass and optimization\u001B[39;00m\n\u001B[0;32m     13\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 14\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m], Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\semantic-segementation\\lib\\site-packages\\torch\\_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    491\u001B[0m     )\n\u001B[1;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\semantic-segementation\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        images, targets = batch['image'].to(device), batch['tensor_category'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Compute your loss based on the weak annotations\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save or use the trained model for inference\n",
    "torch.save(model.state_dict(), 'seeds_weakly_supervised_segmentation_model.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T21:41:41.284844400Z",
     "start_time": "2023-12-17T21:41:17.446836Z"
    }
   },
   "id": "cc34186505a36c6e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
