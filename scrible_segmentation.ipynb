{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-19T00:42:10.578691700Z",
     "start_time": "2023-12-19T00:42:10.510188900Z"
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import ToPILImage\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import relu\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    'person': 0,\n",
    "    'bird': 1,\n",
    "    'cat': 2,\n",
    "    'cow': 3,\n",
    "    'dog': 4,\n",
    "    'horse': 5,\n",
    "    'sheep': 6,\n",
    "    'aeroplane': 7,\n",
    "    'plane': 7,\n",
    "    'bike': 8,\n",
    "    'bicycle':8,\n",
    "    'boat': 9,\n",
    "    'bus': 10,\n",
    "    'car': 11,\n",
    "    'motorbike': 12,\n",
    "    'train': 13,\n",
    "    'bottle': 14,\n",
    "    'chair': 15,\n",
    "    'diningtable': 16,\n",
    "    'table':16,\n",
    "    'pottedplant': 17,\n",
    "    'plant': 17,\n",
    "    'sofa': 18,\n",
    "    'tvmonitor': 19,\n",
    "    'monitor': 19,\n",
    "    'background': 20,\n",
    "    'void': 255\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T00:42:10.619690900Z",
     "start_time": "2023-12-19T00:42:10.521188200Z"
    }
   },
   "id": "1d689f2920cd0a61"
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "def read_scribble_xml(xml_file):\n",
    "    #  Parse XML into Element Tree\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    #  Read meta data\n",
    "    filename = root.find('filename').text\n",
    "    width = int(root.find('size/width').text)\n",
    "    height = int(root.find('size/height').text)\n",
    "\n",
    "    #  Read all points and assign them to the tensor\n",
    "    tensor_categories = torch.zeros((22, 224, 224))\n",
    "    \n",
    "    \n",
    "    polygons = root.findall('polygon')\n",
    "    for polygon in polygons:\n",
    "        tag = polygon.find('tag').text\n",
    "        points = np.array([(min(int(int(point.find('X').text)/width*224), 223), min(int(int(point.find('Y').text)/height*224),223)) for point in polygon.findall('point')])\n",
    "        tensor_categories[class_mapping[tag], points[:, 1], points[:, 0]] = 1 \n",
    "    \n",
    "    return filename, tensor_categories\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T00:42:10.620691800Z",
     "start_time": "2023-12-19T00:42:10.537201600Z"
    }
   },
   "id": "ad03a895eb280c5d"
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "class ScribbleDataset(Dataset):\n",
    "    def __init__(self, image_dir, xml_dir, xml_files, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.xml_files = xml_files\n",
    "        self.transform = transform\n",
    "        self.xml_dir = xml_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xml_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xml_file = self.xml_files[idx]\n",
    "        xml_path = os.path.join(self.xml_dir, xml_file)\n",
    "        image_path = os.path.join(self.image_dir, xml_file.replace(\".xml\", \".jpg\"))\n",
    "\n",
    "        filename, tensor_categories = read_scribble_xml(xml_path)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        sample = {'filename':filename,'image': image, 'tensor_category': tensor_categories}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T00:42:10.620691800Z",
     "start_time": "2023-12-19T00:42:10.553188500Z"
    }
   },
   "id": "ebc222be7c187739"
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        filename, image, tensor_categories = sample['filename'], sample['image'], sample['tensor_category']\n",
    "        \n",
    "        # Convert image to tensor\n",
    "        image = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()])(image)\n",
    "        \n",
    "        return {'filename':filename, 'image': image, 'tensor_category': tensor_categories}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T00:42:10.620691800Z",
     "start_time": "2023-12-19T00:42:10.569189600Z"
    }
   },
   "id": "d01eeb3821ffc4f8"
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "# Set up dataset and dataloader\n",
    "xml_dir = \"scribble\"\n",
    "image_dir = \"train_JPEGImages\"\n",
    "xml_list = os.listdir(xml_dir)\n",
    "transform = transforms.Compose([ToTensor()])\n",
    "train_data, val_data = train_test_split(xml_list, test_size=0.1, random_state=1)\n",
    "train_dataset = ScribbleDataset(image_dir=image_dir, xml_dir=xml_dir, xml_files=train_data, transform=transform)\n",
    "val_dataset = ScribbleDataset(image_dir=image_dir, xml_dir=xml_dir, xml_files=val_data, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T00:42:10.621683800Z",
     "start_time": "2023-12-19T00:42:10.583684600Z"
    }
   },
   "id": "c170f2412f1e9f42"
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "# Define the U-Net model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        resnet18 = models.resnet18(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*list(resnet18.children())[:-2])\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, num_classes, kernel_size=2, stride=2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T00:42:10.621683800Z",
     "start_time": "2023-12-19T00:42:10.600684Z"
    }
   },
   "id": "d85604ddeb079e6c"
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "class Partial_Cross_Entropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Partial_Cross_Entropy, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, scribbles, epsilon=1e-6):\n",
    "        predictions_clamped = torch.clamp(predictions, min=epsilon)\n",
    "\n",
    "        # Select the probabilities where there are scribbles\n",
    "        selected_probabilities = predictions_clamped[scribbles.bool()]\n",
    "\n",
    "        # Calculate the negative log likelihood\n",
    "        loss = -torch.log(selected_probabilities).sum()\n",
    "        \n",
    "        return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T00:42:10.658691200Z",
     "start_time": "2023-12-19T00:42:10.614684300Z"
    }
   },
   "id": "37626899570d4e51"
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "class N_Link_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(N_Link_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, predication):\n",
    "        sigma_sqr = 0.0025\n",
    "        var_lambda = 100\n",
    "        #  Calculate link weight over the batch\n",
    "        n_right_weight = var_lambda*torch.exp(-torch.linalg.norm(torch.roll(inputs, shifts=1, dims=2) - inputs, dim=1) / (2*sigma_sqr))\n",
    "        n_below_weight = var_lambda*torch.exp(-torch.linalg.norm(torch.roll(inputs, shifts=1, dims=3) - inputs, dim=1) / (2*sigma_sqr))\n",
    "\n",
    "        n_link_weight = n_right_weight + n_below_weight\n",
    "        \n",
    "        #  Calculate predication L2 norm over the batch\n",
    "        n_right_coherence = torch.linalg.norm(torch.roll(predication, shifts=1, dims=2) - predication, dim=1)\n",
    "        n_below_coherence = torch.linalg.norm(torch.roll(predication, shifts=1, dims=3) - predication, dim=1)\n",
    "        n_link_coherence = n_right_coherence + n_below_coherence\n",
    "        \n",
    "        n_link_loss = (n_link_coherence*n_link_weight).sum()\n",
    "\n",
    "        return n_link_loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T00:42:10.690690900Z",
     "start_time": "2023-12-19T00:42:10.632684900Z"
    }
   },
   "id": "cad30d4369587b92"
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "class PairwiseRegularizationLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PairwiseRegularizationLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input_img, output_img):\n",
    "        # Ensure input and output have the same spatial dimensions\n",
    "        assert input_img.shape[2:] == output_img.shape[2:], \"Input and output images must have the same spatial dimensions\"\n",
    "        var_lambda = 10000\n",
    "        sigma_sqr = 5\n",
    "        # Calculate pairwise differences for input and output images\n",
    "        right_diff_input = torch.roll(input_img, shifts=1, dims=3) - input_img\n",
    "        below_diff_input = torch.roll(input_img, shifts=1, dims=2) - input_img\n",
    "\n",
    "        right_diff_output = torch.roll(output_img, shifts=-1, dims=3) - output_img\n",
    "        below_diff_output = torch.roll(output_img, shifts=-1, dims=2) - output_img\n",
    "\n",
    "        # Calculate L2 norms\n",
    "        right_norm_input = var_lambda* torch.exp(-1 * torch.linalg.norm(right_diff_input, dim=1) / sigma_sqr)\n",
    "        below_norm_input = var_lambda* torch.exp(-1 * torch.linalg.norm(below_diff_input, dim=1) / sigma_sqr)\n",
    "\n",
    "        right_norm_output = torch.linalg.norm(right_diff_output, dim=1)\n",
    "        below_norm_output = torch.linalg.norm(below_diff_output, dim=1)\n",
    "\n",
    "        # Calculate weighted loss\n",
    "        loss = ((right_norm_output * right_norm_input) + (below_norm_output * below_norm_input)).mean()\n",
    "\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T00:42:10.690690900Z",
     "start_time": "2023-12-19T00:42:10.649684600Z"
    }
   },
   "id": "7d8d14bd92293e44"
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": "UNet(\n  (encoder): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (decoder): Sequential(\n    (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n    (5): ReLU(inplace=True)\n    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Upsample(scale_factor=2.0, mode='bilinear')\n    (13): ConvTranspose2d(64, 22, kernel_size=(2, 2), stride=(2, 2))\n    (14): Softmax(dim=1)\n  )\n)"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize U-Net model\n",
    "in_channels = 3\n",
    "out_channels = 22  # Number of classes\n",
    "model_UNet = UNet(out_channels)\n",
    "num_epochs = 10\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion_1 = nn.CrossEntropyLoss()\n",
    "criterion_2 = N_Link_Loss()\n",
    "criterion_3 = PairwiseRegularizationLoss()\n",
    "criterion_4 = Partial_Cross_Entropy()\n",
    "optimizer = optim.Adam(model_UNet.parameters(), lr=0.001)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model_UNet.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T00:42:10.953686100Z",
     "start_time": "2023-12-19T00:42:10.662685200Z"
    }
   },
   "id": "eaf66f4bc98e230a"
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "# Train function for image dataset\n",
    "def train_image(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "    train_PCE_loss = 0\n",
    "    train_r_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        images, targets = batch['image'].to(device), batch['tensor_category'].to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Compute your loss based on the weak annotation\n",
    "        PCE_loss = criterion_4(outputs, targets)\n",
    "        r_loss= criterion_3(images, outputs)\n",
    "        loss = PCE_loss+r_loss\n",
    "        train_PCE_loss += PCE_loss.item()\n",
    "        train_r_loss += r_loss.item()\n",
    "        train_loss += loss.item()\n",
    "        #print(criterion_4(outputs, targets).item())\n",
    "        #print(criterion_3(images, outputs).item())\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_train_loss = train_loss / num_batches\n",
    "    average_PCE_loss = train_PCE_loss / num_batches\n",
    "    average_r_loss = train_r_loss / num_batches\n",
    "    return average_train_loss, average_PCE_loss, average_r_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T00:42:10.954683600Z",
     "start_time": "2023-12-19T00:42:10.934683200Z"
    }
   },
   "id": "dc624a1e2d20c08c"
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "# Test function for image dataset\n",
    "def test_image(model, dataloader, device):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    test_PCE_loss = 0\n",
    "    test_r_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            images, targets = batch['image'].to(device), batch['tensor_category'].to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            PCE_loss = criterion_4(outputs, targets)\n",
    "            r_loss= criterion_3(images, outputs)\n",
    "            loss = PCE_loss+r_loss\n",
    "            \n",
    "            test_PCE_loss += PCE_loss.item()\n",
    "            test_r_loss += r_loss.item()\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    average_test_loss = test_loss / num_batches\n",
    "    average_PCE_loss = test_PCE_loss / num_batches\n",
    "    average_r_loss = test_r_loss / num_batches\n",
    "    return average_test_loss, average_PCE_loss, average_r_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T00:42:11.055683800Z",
     "start_time": "2023-12-19T00:42:10.949685700Z"
    }
   },
   "id": "cc8d108b2f3579c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  41%|████      | 279/677 [01:30<02:07,  3.12it/s]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_PCE_loss, train_r_loss = train_image(model_UNet, train_dataloader, optimizer, device)\n",
    "    validation_loss, test_PCE_loss, test_r_loss = test_image(model_UNet, val_dataloader, device)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} PCE Loss: {train_PCE_loss:.4f} Regularization Loss: {train_r_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {validation_loss:.4f} PCE Loss: {test_PCE_loss:.4f} Regularization Loss: {test_r_loss:.4f}\")\n",
    "\n",
    "# Save or use the trained model for inference\n",
    "torch.save(model_UNet.state_dict(), 'seeds_weakly_supervised_segmentation_model.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-19T00:42:10.968685Z"
    }
   },
   "id": "cc34186505a36c6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_list = [i for i in range(1, 22)]\n",
    "pascal_voc_colors = [\n",
    "    \"#FF0000\",  # Red\n",
    "    \"#00FF00\",  # Lime\n",
    "    \"#0000FF\",  # Blue\n",
    "    \"#FFFF00\",  # Yellow\n",
    "    \"#00FFFF\",  # Cyan\n",
    "    \"#FF00FF\",  # Magenta\n",
    "    \"#800000\",  # Maroon\n",
    "    \"#808000\",  # Olive\n",
    "    \"#008000\",  # Green\n",
    "    \"#800080\",  # Purple\n",
    "    \"#008080\",  # Teal\n",
    "    \"#000080\",  # Navy\n",
    "    \"#FFA500\",  # Orange\n",
    "    \"#A52A2A\",  # Brown\n",
    "    \"#20B2AA\",  # Light Sea Green\n",
    "    \"#778899\",  # Light Slate Gray\n",
    "    \"#B0C4DE\",  # Light Steel Blue\n",
    "    \"#FFFFE0\",  # Light Yellow\n",
    "    \"#00FA9A\",  # Medium Spring Green\n",
    "    \"#FFD700\",  # Gold\n",
    "    \"#000000\",  # black\n",
    "    \"#FF69B4\"   # Hot Pink\n",
    "]\n",
    "print(len(pascal_voc_colors))\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(class_list, pascal_voc_colors)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7b1f1622dfa280f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_UNet.eval()\n",
    "image_number = 458\n",
    "tensor = model_UNet(val_dataset[image_number]['image'].to(device).unsqueeze(0)).squeeze(0)\n",
    "normalized_tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "predicted_classes = torch.argmax(tensor, dim=0)\n",
    "plt.imshow(predicted_classes.cpu(), cmap=cmap)\n",
    "\n",
    "#plt.imshow(val_dataset[3]['image'].permute(1, 2, 0).numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4cef312a9b45ac94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(val_dataset[image_number]['image'].permute(1, 2, 0).numpy())\n",
    "print(val_dataset[image_number]['filename'])\n",
    "#print(val_dataset[3]['tensor_category'])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "cfc18d0358e20ce7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
